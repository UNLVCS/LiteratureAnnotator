from typing import Iterable
import json
from snorkel.labeling import LabelingFunction
from cross_ref import cross_ref, judge_me
from heuristics import heuristics
from human_scores import researchers_scores
from typing import Dict, Any, Optional, List
from llm_providers import (
    BaseLLMProvider,
    OpenAIProvider, 
    AnthropicProvider, 
    HuggingFaceProvider,
    OllamaProvider,
    Query,
    LLMResponse
)


# Values that a LabelFunction should spit out according to Snorkel
ABSTAIN = -1
INCORRECT = 0
CORRECT = 1

class Rater():
    def __init__(self, model: BaseLLMProvider, query: Query) -> None:
        self.model = model
        self.query = query

    @property
    def name(self) -> str: 
        return self.model.get_provider_name()


    def score_pair(self, generated_label: str, justificiation: str, rubric: Dict[str, Any]) -> Dict[str, float]:
        # Call LLM adapters here 
        self.query['generated_label'] = generated_label
        self.query['justificiation'] = justificiation
        
        score, conf = self.model.call_api(query=self.query)

        return {
                    "Score": score,
                    "Confidence": conf
                }

def cross_ref_judge(model, rater: Rater, target: Dict[Any], rubric: Dict[str, Any]) -> int:
    """
        Returns CORRECT/INCORRECT/ABSTAIN for 'rater judges target' on row x.
            Assumes x has attributes: model_name, inference, justification.
        Note that this is the logic only for the cross_referncing label functions that use LLMs to judge each other's responses
        Logic for other label functions is encapsulated elsewhere
    """

    if model.get_provider_name == target["model"]:
        return ABSTAIN
    
    try:
        out = rater.score_pair(target["generated_label"], target["justification"], rubric)
    except:
        return ABSTAIN
    
    if not out:
        return ABSTAIN

    score = float(out.score, 0.0)
    conf = float(out.confidence, 0.0)


    min_conf = float(rubic_num(rubric, "min_confidence", 0.5))
    thresh   = float(rubic_num(rubric, "pass_threshold", 0.6))

    if conf < min_conf:
        return ABSTAIN
    return CORRECT if score >= thresh else INCORRECT

def rubic_num(rubric: Dict[str, Any], key: str, default: float) -> float:
    v = rubric.get(key, default)
    try:
        return float(v)
    except Exception:
        return default

def make_pair_lf(rater: Rater, target: Dict[Any], rubric: Dict[str, any]) -> LabelingFunction:
    """
        Creates one LF named 'lf_<rater>_judges_<target>' and injects dependencies
    """
    lf_name = f"lf_{rater.name}_judges_{target["model"]}"
    resources = {"rater": rater, "target": target, "rubric": dict[rubric]}
    return LabelingFunction(name = lf_name, f = cross_ref_judge, resources = resources)

def build_pairwise_lfs(raters: Iterable[any], targets: Iterable[Dict], rubric: Dict[str, Any]) -> List[LabelingFunction]:
    lfs: List[LabelingFunction] = []
    raters = list(raters)
    targets = list(targets)

    # Making rater and target pairs. Each rater rates every other target
    """
                                                TARGET Annotations
                                 GPT-4o | Claude | Gemini | GPT-OSS | Qwen | DeepSeek
                        GPT-4o.   abs
                        Claude              abs 
        Rater Models.   Gemini                      abs
                        GPT-oss                             abs
                        Qwen                                          abs
                        DeepSeek                                                abs

        abs means ABSTAIN from rating. If blank then that model will rate the target generated by that model
    """

    for r in raters:
        for t in targets:
            if r.name == t:
                continue
            lfs.append(make_pair_lf(r, t, rubric))


def RaterFactory(params_path: str, query_path: str) -> List[Rater]:

    def init_query(query_path: str) -> Query:
        with open(query_path, 'r') as f:
            query_dict = json.load(f)
        return Query(**query_dict)

    def init_llms(params_path: str) -> List:
        """
        Initialize all LLM providers using the given parameter dictionary.

        Args:
            params (Dict[any]): Dictionary containing parameters for each provider.
                Example:
                    {
                        "openai": {"api_key": "...", "model": "...", ...},
                        "anthropic": {"api_key": "...", "model": "...", ...},
                        "huggingface": {"api_key": "...", "model": "...", ...}
                    }

        Returns:
            List: List of initialized LLM provider instances.
        """
        with open(params_path, 'r') as f:
            params = json.load(f)
            
        llms = []
        # if "openai" in params:
        #     openai_params = params["openai"]
        #     llms.append(OpenAIProvider(**openai_params))
        # if "anthropic" in params:
        #     anthropic_params = params["anthropic"]
        #     llms.append(AnthropicProvider(**anthropic_params))
        # if "huggingface" in params:
        #     hf_params = params["huggingface"]
        #     llms.append(HuggingFaceProvider(**hf_params))
        # if "ollama" in params:
        #     ollama_params = params["ollama"]
        #     llms.append(OllamaProvider(**ollama_params))
        for provider in params.values():
            for model in provider:
                if model['skip']: continue
                if "openai" == provider:
                    openai_params = model
                    llms.append(OpenAIProvider(**openai_params))
                if "anthropic" == provider:
                    anthropic_params = model
                    llms.append(AnthropicProvider(**anthropic_params))
                if "huggingface" == provider:
                    hf_params = model
                    llms.append(HuggingFaceProvider(**hf_params))
                if "ollama" == provider:
                    ollama_params = model
                    llms.append(OllamaProvider(**ollama_params))
        return llms

    llms = init_llms(params_path=params_path)
    query = init_query(query_path=query_path)

    ratersList = []
    for llm in llms:
        rater = Rater(model=llm, query=query)
        ratersList.append(rater)
# @labeling_function('CrossReferencer', cross_ref, {})
# def cross_ref_lf():
#     pass

# @labeling_function('Heuristics', heuristics, {})
# def heuristics_lf():
#     pass

# @labeling_function('ResearchersScores', researchers_scores, {})
# def researcher_scores():
#     pass


if __name__ == "__main__":
    raters = RaterFactory(params_path='./', query_path='./')